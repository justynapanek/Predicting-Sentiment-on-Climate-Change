{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,GetOldTweets3,IPython,pandas,sqlite3,time,urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a sqlite database to store tweets downloaded\n",
    "sqlite3 is included in Python's packages. It can create an SQL database that runs locally without a server. The benefit of using sqlite is that the SQL commands are not saved in the database until 'commit()' function is called. This is useful when working with downloads since sometimes the connection can be lost in the middle of the script and cause corruption to the data. Using 'commit()' in sqlite only after the downloads is complete safeguards against such data corruption incidents. The date last downloaded is also saved in such a database so that the download job can be interrupted and continued with ease.\n",
    ">Previous runs have encountered this issue of tweets producing an error during GetOldTweets3 tweet-cleaning function.\n",
    ">https://github.com/Jefferson-Henrique/GetOldTweets-python/issues/163\n",
    ">The solution is to download the tweet as-is and process the cleaning afterwards.\n",
    ">The following edit was done to bypass the tweet-cleaning function:\n",
    ">Edit from line 87 of '\\site-packages\\GetOldTweets3\\manager\\TweetManager.py'\n",
    "><br>\n",
    "><br>tweet.to = usernames[1] if len(usernames) >= 2 else None  # take the first recipient if many\n",
    "><br>Comment out this --> rawtext = TweetManager.textify(tweetPQ(\"p.js-tweet-text\").html(), tweetCriteria.emoji)\n",
    "><br>Comment out this --> tweet.text = re.sub(r\"\\s+\", \" \", rawtext).replace('# ', '#').replace('@ ', '@').replace('$ ', '$')\n",
    "><br>Add this --> tweet.text = tweetPQ(\"p.js-tweet-text\").html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates the [date_range] table with the date we intend to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should not be run if we wish to continue from where we left off (e.g. connection is lost midway during download).\n",
    "conn01 = sqlite3.connect('twitter_us.db')\n",
    "cur_sqlite = conn01.cursor()\n",
    "\n",
    "cur_sqlite.execute('CREATE TABLE IF NOT EXISTS [date_range] ([since],[until]);')\n",
    "\n",
    "cur_sqlite.execute('''\n",
    "DELETE FROM [date_range];''')\n",
    "\n",
    "cur_sqlite.execute('''\n",
    "INSERT INTO [date_range]\n",
    "VALUES(\"2020-04-01\",\"2020-04-02\");''')\n",
    "\n",
    "conn01.commit()\n",
    "conn01.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/connect to twitter_us.db. The file will be created if it doesn't exist.\n",
    "conn01 = sqlite3.connect('twitter_us.db',\n",
    "                         detect_types = sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES) # this line is for storing dates in sqlite\n",
    "cur_sqlite = conn01.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-01-01 Start-> 01:03\n",
      "Alabama\n",
      "Alaska\n",
      "Arizona\n",
      "Arkansas\n",
      "California\n",
      "Colorado\n",
      "Connecticut\n",
      "Delaware\n",
      "Florida\n",
      "Georgia\n",
      "Hawaii\n",
      "Idaho\n",
      "Illinois\n",
      "Indiana\n",
      "Iowa\n",
      "Kansas\n",
      "Kentucky\n",
      "Louisiana\n",
      "Maine\n",
      "Maryland\n",
      "Massachusetts\n",
      "1--32-->32\n",
      "Michigan\n",
      "Minnesota\n",
      "Mississippi\n",
      "Missouri\n",
      "Montana\n",
      "Nebraska\n",
      "Nevada\n",
      "New Hampshire\n",
      "New Jersey\n",
      "New Mexico\n",
      "New York\n",
      "North Carolina\n",
      "North Dakota\n",
      "Ohio\n",
      "Oklahoma\n",
      "Oregon\n",
      "Pennsylvania\n",
      "Rhode Island\n",
      "South Carolina\n",
      "South Dakota\n",
      "Tennessee\n",
      "Texas\n",
      "Utah\n",
      "Vermont\n",
      "Virginia\n",
      "Washington\n",
      "West Virginia\n",
      "Wisconsin\n",
      "Wyoming\n",
      "2011-01-01 Done-> 01:05\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now()\n",
    "tweet_volume = 0\n",
    "tweet_batch = 1800\n",
    "tweet_states = ''\n",
    "run_days = 3650\n",
    "\n",
    "tweet_attr = ['id',\n",
    "              'permalink',\n",
    "              'username',\n",
    "              'to',\n",
    "              'text',\n",
    "              'date',\n",
    "              'replies',\n",
    "              'retweets',\n",
    "              'favorites',\n",
    "              'mentions',\n",
    "              'hashtags',\n",
    "              'geo']\n",
    "\n",
    "us_states = ['Alabama',\n",
    "             'Alaska',\n",
    "             'Arizona',\n",
    "             'Arkansas',\n",
    "             'California',\n",
    "             'Colorado',\n",
    "             'Connecticut',\n",
    "             'Delaware',\n",
    "             'Florida',\n",
    "             'Georgia',\n",
    "             'Hawaii',\n",
    "             'Idaho',\n",
    "             'Illinois',\n",
    "             'Indiana',\n",
    "             'Iowa',\n",
    "             'Kansas',\n",
    "             'Kentucky',\n",
    "             'Louisiana',\n",
    "             'Maine',\n",
    "             'Maryland',\n",
    "             'Massachusetts',\n",
    "             'Michigan',\n",
    "             'Minnesota',\n",
    "             'Mississippi',\n",
    "             'Missouri',\n",
    "             'Montana',\n",
    "             'Nebraska',\n",
    "             'Nevada',\n",
    "             'New Hampshire',\n",
    "             'New Jersey',\n",
    "             'New Mexico',\n",
    "             'New York',\n",
    "             'North Carolina',\n",
    "             'North Dakota',\n",
    "             'Ohio',\n",
    "             'Oklahoma',\n",
    "             'Oregon',\n",
    "             'Pennsylvania',\n",
    "             'Rhode Island',\n",
    "             'South Carolina',\n",
    "             'South Dakota',\n",
    "             'Tennessee',\n",
    "             'Texas',\n",
    "             'Utah',\n",
    "             'Vermont',\n",
    "             'Virginia',\n",
    "             'Washington',\n",
    "             'West Virginia',\n",
    "             'Wisconsin',\n",
    "             'Wyoming']\n",
    "\n",
    "SQL_str = ''\n",
    "\n",
    "for i in tweet_attr:\n",
    "    SQL_str += '['+i+']'\n",
    "    if i == tweet_attr[-1]:\n",
    "        pass\n",
    "    elif i == 'date':\n",
    "        SQL_str += ' timestamp,'\n",
    "    else:\n",
    "        SQL_str += ','\n",
    "\n",
    "# Create a [twitter] table where the tweets will be saved in.\n",
    "cur_sqlite.execute('''\n",
    "CREATE TABLE IF NOT EXISTS [twitter] (\n",
    "[US],\n",
    "'''+SQL_str+''');''')\n",
    "\n",
    "# Create a temporary table for the tweets to be stored before being saved into the actual [twitter] table.\n",
    "cur_sqlite.execute('''\n",
    "CREATE TABLE IF NOT EXISTS [temp] (\n",
    "[US],\n",
    "'''+SQL_str+''');''')\n",
    "\n",
    "# This function inserts the tweets into the [temp] table as the tweet is downloaded one tweet at a time.\n",
    "# This function can be called from the GetOldTweets3 downloading function.\n",
    "def tweet_buffer(tweets):\n",
    "    global time_start\n",
    "    global tweet_volume\n",
    "    global tweet_states\n",
    "    tweet_volume += len(tweets)\n",
    "\n",
    "    for i in tweets:\n",
    "        cur_sqlite.execute('''\n",
    "INSERT INTO [temp]\n",
    "    VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?);''',\n",
    "                           (tweet_states,\n",
    "                            i.id,\n",
    "                            i.permalink,\n",
    "                            i.username,\n",
    "                            i.to,\n",
    "                            i.text,\n",
    "                            i.date,\n",
    "                            i.replies,\n",
    "                            i.retweets,\n",
    "                            i.favorites,\n",
    "                            i.mentions,\n",
    "                            i.hashtags,\n",
    "                            i.geo))\n",
    "\n",
    "# When the tweet volume reaches a certain number, twitter website will start to restrict the connection from downloading more tweets for a while.\n",
    "# To prevent that, the script needs to wait for a while after downloading several scripts.\n",
    "# This wait time will be shorter than if the twitter webite restricts the connection.\n",
    "# While waiting, it is a good time to save the data into [twitter] table.\n",
    "    if tweet_volume+tweet_batch >= 14000 or len(tweets) < tweet_batch:\n",
    "        cur_sqlite.execute('''\n",
    "DELETE FROM [twitter]\n",
    "WHERE\n",
    "    [id] IN (\n",
    "SELECT\n",
    "    [id]\n",
    "FROM [temp]);''')\n",
    "\n",
    "        cur_sqlite.execute('''\n",
    "INSERT INTO [twitter]\n",
    "SELECT\n",
    "    *\n",
    "FROM [temp];''')\n",
    "\n",
    "        cur_sqlite.execute('''\n",
    "DELETE FROM [temp];''')\n",
    "\n",
    "        time_lapse = (datetime.datetime.now()-time_start).seconds\n",
    "        if time_lapse < tweet_volume/12.:\n",
    "            time.sleep((tweet_volume/12.)-time_lapse)\n",
    "        else:\n",
    "            pass\n",
    "        print(str(tweet_volume)+'--'+str(time_lapse)+'-->'+str((datetime.datetime.now()-time_start).seconds))\n",
    "        time_start = datetime.datetime.now()\n",
    "        tweet_volume = 0\n",
    "    else:\n",
    "        time_lapse = (datetime.datetime.now()-time_start).seconds\n",
    "        print(str(tweet_volume)+'--'+str(time_lapse))\n",
    "\n",
    "# This is where the data is actually saved into the database. Previously all SQL commands will be 'saved' in a temporary file until this step.\n",
    "    conn01.commit()\n",
    "\n",
    "# This is the actual loop that calls the GetOldTweet3 functions to download the tweets.\n",
    "# Test connection is active. If it is, run the script. Otherwise, wait for 1 minute then try again.\n",
    "for i in range(0,\n",
    "               run_days):\n",
    "    try:\n",
    "        urllib.request.urlopen(r'http://www.google.com')\n",
    "        time_start = datetime.datetime.now()\n",
    "        tweet_volume = 0\n",
    "        tweet_batch = 1800\n",
    "        tweet_states = ''\n",
    "\n",
    "        IPython.display.clear_output()\n",
    "\n",
    "# This is where we read the last downloaded date from the [date_range] table. We will continue the download from this date.\n",
    "        df = pandas.read_sql('''\n",
    "SELECT\n",
    "    *\n",
    "FROM [date_range];''',\n",
    "                             con = conn01)\n",
    "\n",
    "        print(df['since'][0]+' Start-> '+datetime.datetime.now().strftime('%H:%M'))\n",
    "\n",
    "# Clear the [temp] table. This is for incidences where we have to continue after the connection was lost and we have to start again.\n",
    "        cur_sqlite.execute('''\n",
    "DELETE FROM [temp];''')\n",
    "\n",
    "# For every US state, search for and download the tweets using the keywords specified in 'setQuerySearch()'.\n",
    "        for j in us_states:\n",
    "            tweet_states = j\n",
    "            print(tweet_states)\n",
    "            criteria = GetOldTweets3.manager.TweetCriteria().setQuerySearch('global warming')\\\n",
    "                                                            .setSince(df['since'][0])\\\n",
    "                                                            .setUntil(df['until'][0])\\\n",
    "                                                            .setNear(tweet_states)\n",
    "\n",
    "# The TweetManager is where the action is. Tweets are getting parsed and downloaded here.\n",
    "            GetOldTweets3.manager.TweetManager.getTweets(tweetCriteria = criteria,\n",
    "                                                         receiveBuffer = tweet_buffer,\n",
    "                                                         bufferLength = tweet_batch)\n",
    "\n",
    "            time.sleep(1)\n",
    "        print(df['since'][0]+' Done-> '+datetime.datetime.now().strftime('%H:%M'))\n",
    "\n",
    "# Subtract 1 day to the date in [date_range] table. We have downloaded 1 day of tweet. 'commit()' is called after this.\n",
    "# We move from most recent tweets to older tweets.\n",
    "        df = pandas.read_sql('''\n",
    "SELECT\n",
    "    STRFTIME(\"%Y-%m-%d\",\n",
    "             [since],\n",
    "             \"-1 day\") AS [since],\n",
    "    STRFTIME(\"%Y-%m-%d\",\n",
    "             [until],\n",
    "             \"-1 day\") AS [until]\n",
    "FROM [date_range];''',\n",
    "                             con = conn01)\n",
    "\n",
    "        cur_sqlite.execute('''\n",
    "DELETE FROM [date_range];''')\n",
    "\n",
    "        cur_sqlite.execute('''\n",
    "INSERT INTO [date_range]\n",
    "    VALUES(?,?);''',\n",
    "                           (df['since'][0],\n",
    "                            df['until'][0]))\n",
    "        conn01.commit()\n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_sql('''\n",
    "SELECT\n",
    "    [US],\n",
    "    [id],\n",
    "    [permalink],\n",
    "    [username],\n",
    "    [to],\n",
    "    [text],\n",
    "    STRFTIME(\"%Y-%m-%d %H:%M:%S\",[date]) AS [date],\n",
    "    [replies],\n",
    "    [retweets],\n",
    "    [favorites],\n",
    "    [mentions],\n",
    "    [hashtags],\n",
    "    [geo]\n",
    "FROM [twitter]''',\n",
    "                     con = conn01)\n",
    "\n",
    "df['date'] = pandas.to_datetime(df['date'])\n",
    "\n",
    "df.to_csv(r'twitter_us.csv',\n",
    "          index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn01.commit()\n",
    "conn01.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
